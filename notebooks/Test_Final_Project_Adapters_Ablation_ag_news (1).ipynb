{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTDEKPqt7wd3",
        "outputId": "551bbf4f-bff0-4914-cffd-96bc8776343f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "d3ccQxBAF1cV",
        "outputId": "0e2b6994-082e-4baf-fd51-3118f1a3ca23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8OWIn5c_xWu"
      },
      "source": [
        "# Install the necessary packages to run this script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N1iX_4B7-oO0",
        "outputId": "dd0a98b2-9c9c-42d6-fc40-013d154d95bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch scikit-learn numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeTXGAA1_5dl"
      },
      "source": [
        "### Load the necessary packages to run the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e9wH5bYVefhA"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, RobertaConfig\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from modifiers.ModifiedRobertaWithAdaptersV5 import ModifiedRobertaForSequenceClassification\n",
        "from transformers import TrainerCallback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CslQ00wNefhE"
      },
      "source": [
        "Now lets check your GPU availability and load some sanity checkers. By default you should be using your gpu for this assignment if you have one available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHOKA0U5efhF",
        "outputId": "142bbe79-eb69-45f7-9576-a05a9a059daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check device availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"You are using device: %s\" % device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDmu06h_efhI"
      },
      "source": [
        "## **1.2: Load Data**\n",
        "Loading the ag_news dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "LSZ-H2I8_mvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2caba49d-05ee-487d-888a-8a29d599fbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load AG_NEWS dataset\n",
        "dataset = load_dataset(\"fancyzhx/ag_news\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tn0sOcH6Sw3u"
      },
      "outputs": [],
      "source": [
        "# load the tokenizer for Roberta\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3oNSSDVrXEQM"
      },
      "outputs": [],
      "source": [
        "# define the function for tokenizing the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding = \"max_length\", truncation= True, max_length= 512)\n",
        "\n",
        "# Apply the tokenizer to the datasets\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched= True)\n",
        "\n",
        "# Set the format of the dataset to return PyTorch tensors\n",
        "tokenized_datasets = tokenized_dataset.remove_columns([\"text\"])\n",
        "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "test_dataset = tokenized_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sJeKmx3jlc_b"
      },
      "outputs": [],
      "source": [
        "small_train_dataset = train_dataset.select(range(1000))\n",
        "small_test_dataset = test_dataset.select(range(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uuoOcmXA5lb"
      },
      "source": [
        "Setting the base model and the metric function to be used to evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cDXWyHQCaiN",
        "outputId": "4f873781-71f6-4935-bb38-b3a62daad8d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# # Load the pre-trained RoBERTa model\n",
        "base_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
        "# def compute_metrics(p):\n",
        "#     predictions, labels = p\n",
        "#     predictions = torch.argmax(torch.tensor(predictions), dim=-1)\n",
        "#     accuracy = accuracy_score(labels, predictions)\n",
        "#     return {'accuracy': accuracy}\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)  # Get the predicted class by finding the index of the max logit\n",
        "\n",
        "    # Accuracy\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    # Precision, Recall, F1 Score (using macro, micro, or weighted average)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, zero_division=0.0, average=\"weighted\")\n",
        "\n",
        "    # # Confusion matrix (for multi-class, it returns a matrix)\n",
        "    # cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    # Extract True Positives, False Positives, etc. for each class\n",
        "    # Here, we will return a dictionary with confusion matrix components per class (if needed)\n",
        "    # tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (None, None, None, None)\n",
        "\n",
        "    # For multi-class, it's more useful to look at the entire confusion matrix\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        # 'confusion_matrix': cm.tolist(),  # Return the confusion matrix for more insight\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsNweNb6efhf"
      },
      "source": [
        "Setting the training options and hyper-parameter settings. This is going to be the same across all experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjp-pEcyWEDL",
        "outputId": "be3ff5a5-865b-457f-c9d4-2d295b393984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    run_name=\"roberta_ag_news_ablation_adapter\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=128,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.001,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",               # Save checkpoints every epoch\n",
        "    load_best_model_at_end=True,         # Load best model after training\n",
        "    metric_for_best_model=\"accuracy\",    # Metric to monitor\n",
        "    fp16=True,\n",
        "    # report_to=[\"none\"],  # Disable W&B logging\n",
        "    #logging_dir='./logs',\n",
        "    #logging_steps=10,\n",
        "    # report_to=[\"none\"],  # Disable W&B logging\n",
        "    # fp16=True  # Enables mixed precision\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNuBjpoaJEBY"
      },
      "source": [
        "# Please run this below cell when we want to train on all the data. Current setup is just to test if the training is working\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_03_ROriI9AR"
      },
      "outputs": [],
      "source": [
        "trainAllData = True\n",
        "if trainAllData:\n",
        "  small_train_dataset = train_dataset\n",
        "  small_test_dataset = test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlYODagYBfu7"
      },
      "source": [
        "The Following section setup the different types of Models for training on the ag_news dataset\n",
        "# Base model with the classification head is finetuned.\n",
        "The whole model is finetuned to ag_news dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zQuFhYRmENfe",
        "outputId": "3233e1f1-1e60-4d5d-cb5e-d2b98a9409e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# This will display the model structure and the layer structure\n",
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mkU2k4_5BVyj"
      },
      "outputs": [],
      "source": [
        "training_logs = []\n",
        "\n",
        "class CustomCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        logs = logs or {}\n",
        "        if any(key.startswith(\"eval_\") for key in logs):\n",
        "            training_logs.append({\n",
        "                \"epoch\": state.epoch,\n",
        "                \"loss\": logs.get(\"eval_loss\", None),  # Get loss if available, otherwise None\n",
        "                \"accuracy\": logs.get(\"eval_accuracy\", None)\n",
        "            })\n",
        "\n",
        "trainer_base = Trainer(\n",
        "    model=base_model,                         # the model to be trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=small_train_dataset,  # training dataset\n",
        "    eval_dataset=small_test_dataset,   # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # function for computing metrics\n",
        ")\n",
        "\n",
        "trainer_base.add_callback(CustomCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "2KsRUwGxCDBv",
        "outputId": "10e4be16-9870-4ef8-b03e-e798212a4ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgtechankur\u001b[0m (\u001b[33mgtechankur-geaorgia-tech\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/wandb/run-20241207_163102-9psdmby8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gtechankur-geaorgia-tech/huggingface/runs/9psdmby8' target=\"_blank\">roberta_ag_news_ablation_adapter</a></strong> to <a href='https://wandb.ai/gtechankur-geaorgia-tech/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gtechankur-geaorgia-tech/huggingface' target=\"_blank\">https://wandb.ai/gtechankur-geaorgia-tech/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gtechankur-geaorgia-tech/huggingface/runs/9psdmby8' target=\"_blank\">https://wandb.ai/gtechankur-geaorgia-tech/huggingface/runs/9psdmby8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37500/37500 49:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.255700</td>\n",
              "      <td>0.225693</td>\n",
              "      <td>0.933684</td>\n",
              "      <td>0.933910</td>\n",
              "      <td>0.933684</td>\n",
              "      <td>0.933732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.193400</td>\n",
              "      <td>0.297547</td>\n",
              "      <td>0.926184</td>\n",
              "      <td>0.928893</td>\n",
              "      <td>0.926184</td>\n",
              "      <td>0.926511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.202100</td>\n",
              "      <td>0.232183</td>\n",
              "      <td>0.941184</td>\n",
              "      <td>0.941611</td>\n",
              "      <td>0.941184</td>\n",
              "      <td>0.941165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.138100</td>\n",
              "      <td>0.232070</td>\n",
              "      <td>0.945789</td>\n",
              "      <td>0.946259</td>\n",
              "      <td>0.945789</td>\n",
              "      <td>0.945886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.072500</td>\n",
              "      <td>0.240734</td>\n",
              "      <td>0.946579</td>\n",
              "      <td>0.946637</td>\n",
              "      <td>0.946579</td>\n",
              "      <td>0.946575</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=37500, training_loss=0.19198792372385662, metrics={'train_runtime': 2969.4752, 'train_samples_per_second': 202.056, 'train_steps_per_second': 12.628, 'total_flos': 1.578694680576e+17, 'train_loss': 0.19198792372385662, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "trainer_base.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_model.save_pretrained(\"/content/drive/My Drive/1_base_model_ag_news_ablation_with_adapter\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9KSVCGxH0q4",
        "outputId": "788ca7de-2bbc-4c17-e0dc-462094c8f224"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrNc7tEoCuoK"
      },
      "source": [
        "# Base model only the classification head is finetuned.\n",
        "The classification head is finetuned to ag_news dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YKciAk8cC_LK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b19612-ef73-48fb-e5de-a52e69f33902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "base_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
        "model = base_model\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False  # Freeze the encoder layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6qb1qd5JDKG7"
      },
      "outputs": [],
      "source": [
        "trainer_base_ch_only = Trainer(\n",
        "    model=model,                         # the model to be trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=small_train_dataset,  # training dataset\n",
        "    eval_dataset=small_test_dataset,   # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # function for computing metrics\n",
        ")\n",
        "trainer_base_ch_only.add_callback(CustomCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JGdXqMV6DPyD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "af724d6e-c8bb-4cc2-9393-fbae8df87cf7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37500/37500 18:35, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.432000</td>\n",
              "      <td>0.325215</td>\n",
              "      <td>0.891447</td>\n",
              "      <td>0.891696</td>\n",
              "      <td>0.891447</td>\n",
              "      <td>0.891203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.375100</td>\n",
              "      <td>0.302106</td>\n",
              "      <td>0.899737</td>\n",
              "      <td>0.899799</td>\n",
              "      <td>0.899737</td>\n",
              "      <td>0.899661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.376800</td>\n",
              "      <td>0.295422</td>\n",
              "      <td>0.901447</td>\n",
              "      <td>0.901463</td>\n",
              "      <td>0.901447</td>\n",
              "      <td>0.901383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.401900</td>\n",
              "      <td>0.292357</td>\n",
              "      <td>0.901842</td>\n",
              "      <td>0.901574</td>\n",
              "      <td>0.901842</td>\n",
              "      <td>0.901649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.361700</td>\n",
              "      <td>0.291549</td>\n",
              "      <td>0.902105</td>\n",
              "      <td>0.901877</td>\n",
              "      <td>0.902105</td>\n",
              "      <td>0.901923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=37500, training_loss=0.430357732035319, metrics={'train_runtime': 1115.3104, 'train_samples_per_second': 537.967, 'train_steps_per_second': 33.623, 'total_flos': 1.578694680576e+17, 'train_loss': 0.430357732035319, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "trainer_base_ch_only.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model.save_pretrained(\"/content/drive/My Drive/2_base_model_ag_news_ablation_with_adapter\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bICd2WCQIPon",
        "outputId": "f2fdd9d6-92a3-4efe-96ea-b61e93c40167"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjbOPynEDYns"
      },
      "source": [
        "# Custom model with no adapters used. All parameters fine tuned\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "id": "Jy8XKTkqDmDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f94e25c-188d-42ad-8ef1-5c7f47f55cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModifiedRobertaForSequenceClassification(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (modified_layers): ModuleList(\n",
              "    (0-11): 12 x ModifiedRobertaLayer(\n",
              "      (adapter1): PassThroughBlock()\n",
              "      (adapter2): PassThroughBlock()\n",
              "      (attention): RobertaAttention(\n",
              "        (self): RobertaSdpaSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (output): RobertaSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): RobertaIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        (intermediate_act_fn): GELUActivation()\n",
              "      )\n",
              "      (output): RobertaOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "base_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
        "custom_model = ModifiedRobertaForSequenceClassification(base_model ,num_labels=4, adapter_hidden_dim=64, freeze_params = True, adapter_layers = [False] * 12)\n",
        "# View the updated custom model. All encoder blocks should be similar and should have a pass through block instead of the adapter block\n",
        "custom_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PpV_7fxWD6uV"
      },
      "outputs": [],
      "source": [
        "trainer_custom_model = Trainer(\n",
        "    model=custom_model,                         # the model to be trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=small_train_dataset,  # training dataset\n",
        "    eval_dataset=small_test_dataset,   # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # function for computing metrics\n",
        ")\n",
        "\n",
        "trainer_custom_model.add_callback(CustomCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DDj-PZKqEA9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "aba2a674-d891-480d-86c0-f0f83e6baebb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [37500/37500 47:17, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.390200</td>\n",
              "      <td>1.386475</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.390200</td>\n",
              "      <td>1.386475</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.387300</td>\n",
              "      <td>1.386475</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.387500</td>\n",
              "      <td>1.386475</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.386900</td>\n",
              "      <td>1.385986</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=37500, training_loss=1.3881010896809896, metrics={'train_runtime': 2837.9369, 'train_samples_per_second': 211.421, 'train_steps_per_second': 13.214, 'total_flos': 0.0, 'train_loss': 1.3881010896809896, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "trainer_custom_model.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "custom_model.save_pretrained(\"/content/drive/My Drive/3_base_custom_model_ag_news_ablation_with_adapter\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "XBuoE3dfIW9f",
        "outputId": "447c2682-175d-478f-8966-58fb50d3d34d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ModifiedRobertaForSequenceClassification' object has no attribute 'save_pretrained'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-1ca943405408>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/3_base_custom_model_ag_news_ablation_with_adapter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ModifiedRobertaForSequenceClassification' object has no attribute 'save_pretrained'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrHdgWsHEndx"
      },
      "source": [
        "# Custom model with no adapters used. Classification head fine tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XDIQw_lxi-jQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cea54dd-0418-473d-f630-1e19e0305d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# set-requires_grad will freeze all original layer parameters and only train the adapters and classification heads\n",
        "base_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
        "custom_model = ModifiedRobertaForSequenceClassification(base_model ,num_labels=4, adapter_hidden_dim=64, freeze_params = True, adapter_layers = [False] * 12)\n",
        "custom_model.set_requires_grad(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mFM18HPyBT1s"
      },
      "outputs": [],
      "source": [
        "trainer_custom_model_ch_only = Trainer(\n",
        "    model=custom_model,                         # the model to be trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=small_train_dataset,  # training dataset\n",
        "    eval_dataset=small_test_dataset,   # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # function for computing metrics\n",
        ")\n",
        "\n",
        "trainer_custom_model_ch_only.add_callback(CustomCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdKK8MdaE1yi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "1f82720c-6e3d-4d02-fc37-446296044436"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9518' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 9518/37500 04:22 < 12:53, 36.20 it/s, Epoch 1.27/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.940200</td>\n",
              "      <td>1.135082</td>\n",
              "      <td>0.532895</td>\n",
              "      <td>0.634459</td>\n",
              "      <td>0.532895</td>\n",
              "      <td>0.517040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_custom_model_ch_only.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "custom_model.save_pretrained(\"/content/drive/My Drive/4_custom_model_ag_news_ablation_with_adapter\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5h4eFpEQIghv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFvaV0RpGKzw"
      },
      "source": [
        "# Custom model with all layers with adapters used. Adapters and Classification head fine tuned only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WNsQ8BCRGU_X"
      },
      "outputs": [],
      "source": [
        "base_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
        "model_all_adapters = ModifiedRobertaForSequenceClassification(base_model ,num_labels=4, adapter_hidden_dim=64, freeze_params = True, adapter_layers = [True] * 12)\n",
        "# View the updated custom model. All encoder blocks should be similar and should have a pass through block instead of the adapter block\n",
        "model_all_adapters.set_requires_grad(False)\n",
        "model_all_adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G9mKeH4Glk5"
      },
      "outputs": [],
      "source": [
        "trainer_model_all_adapters = Trainer(\n",
        "    model=model_all_adapters,                         # the model to be trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=small_train_dataset,  # training dataset\n",
        "    eval_dataset=small_test_dataset,   # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # function for computing metrics\n",
        ")\n",
        "\n",
        "trainer_model_all_adapters.add_callback(CustomCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1ByeytooGtcY"
      },
      "outputs": [],
      "source": [
        "trainer_model_all_adapters.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_all_adapters.save_pretrained(\"/content/drive/My Drive/5_model_all_adapters_ag_news_ablation_with_adapter\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CcEV0anWIszk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85-daLzrHQM6"
      },
      "source": [
        "# Custom model with lower layers(2) with adapters used. Adapters and Classification head fine tuned only\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1xqzy8HCHV9D"
      },
      "outputs": [],
      "source": [
        "adpt_lyrs = [False]*12\n",
        "adpt_lyrs[0:2] = [True]*2\n",
        "base_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
        "model_lower_adapters = ModifiedRobertaForSequenceClassification(base_model ,num_labels=4, adapter_hidden_dim=64, freeze_params = True, adapter_layers = adpt_lyrs)\n",
        "model_lower_adapters.set_requires_grad(False)\n",
        "# View the updated custom model. All encoder blocks should be similar and should have a pass through block instead of the adapter block\n",
        "model_lower_adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaBNaNzZHrqk"
      },
      "outputs": [],
      "source": [
        "trainer_model_lower_adapters = Trainer(\n",
        "    model=model_lower_adapters,                         # the model to be trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=small_train_dataset,  # training dataset\n",
        "    eval_dataset=small_test_dataset,   # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # function for computing metrics\n",
        ")\n",
        "\n",
        "trainer_model_lower_adapters.add_callback(CustomCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NvdPtxUPHxTl"
      },
      "outputs": [],
      "source": [
        "trainer_model_lower_adapters.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_lower_adapters.save_pretrained(\"/content/drive/My Drive/6_model_lower_adapters_ag_news_ablation_with_adapter\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Kh-hiFXDI2Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKxWjkmJH5Uj"
      },
      "source": [
        "# Custom model with higher layers(2) with adapters used. Adapters and Classification head fine tuned only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RK7GrJChIO6G"
      },
      "outputs": [],
      "source": [
        "adpt_lyrs = [False]*12\n",
        "adpt_lyrs[10:] = [True]*2\n",
        "base_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
        "model_higher_adapters = ModifiedRobertaForSequenceClassification(base_model ,num_labels=4, adapter_hidden_dim=64, freeze_params = True, adapter_layers = adpt_lyrs)\n",
        "model_higher_adapters.set_requires_grad(False)\n",
        "# View the updated custom model. All encoder blocks should be similar and should have a pass through block instead of the adapter block\n",
        "model_higher_adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKUVjZsBIYyN"
      },
      "outputs": [],
      "source": [
        "trainer_model_higher_adapters = Trainer(\n",
        "    model=model_higher_adapters,                         # the model to be trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=small_train_dataset,  # training dataset\n",
        "    eval_dataset=small_test_dataset,   # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # function for computing metrics\n",
        ")\n",
        "\n",
        "trainer_model_higher_adapters.add_callback(CustomCallback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1mToSfSIdKm"
      },
      "outputs": [],
      "source": [
        "trainer_model_higher_adapters.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_higher_adapters.save_pretrained(\"/content/drive/My Drive/7_model_higher_adapters_ag_news_ablation_with_adapter\")\n",
        "\n"
      ],
      "metadata": {
        "id": "K2JKbn2HI91u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wF8sQ-4JbCU4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "interpreter": {
      "hash": "0e75f62e3678e2cc45ba815b06d45149f3ef8e725365fb50a06024c1d0abc38d"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}